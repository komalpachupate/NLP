{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bag of Words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a “bag” of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
    "\n",
    "It involves three operations:\n",
    "    \n",
    "1) Tokenization:\n",
    "    First, the input text is tokenized. A sentence is represented as a list of its constituent words, and it’s done \n",
    "    for all the input sentences.\n",
    "    \n",
    "2) Vocabulary creation:\n",
    "    Of all the obtained tokenized words, only unique words are selected to create the vocabulary and then sorted by  \n",
    "    alphabetical order.\n",
    "    \n",
    "3) Vector creation\n",
    "    Finally, a sparse matrix is created for the input, out of the frequency of vocabulary words. In this sparse matrix, \n",
    "    each row is a sentence vector whose length (the columns of the matrix) is equal to the size of the vocabulary.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "sents = ['coronavirus is a highly infectious disease',\n",
    "   'coronavirus affects older people the most',\n",
    "   'older people are at high risk due to this disease']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X = cv.fit_transform(sents)\n",
    "X = X.toarray()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affects',\n",
       " 'are',\n",
       " 'at',\n",
       " 'coronavirus',\n",
       " 'disease',\n",
       " 'due',\n",
       " 'high',\n",
       " 'highly',\n",
       " 'infectious',\n",
       " 'is',\n",
       " 'most',\n",
       " 'older',\n",
       " 'people',\n",
       " 'risk',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affects older',\n",
       " 'are at',\n",
       " 'at high',\n",
       " 'coronavirus affects',\n",
       " 'coronavirus is',\n",
       " 'due to',\n",
       " 'high risk',\n",
       " 'highly infectious',\n",
       " 'infectious disease',\n",
       " 'is highly',\n",
       " 'older people',\n",
       " 'people are',\n",
       " 'people the',\n",
       " 'risk due',\n",
       " 'the most',\n",
       " 'this disease',\n",
       " 'to this']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 = CountVectorizer(ngram_range=(2,2))  #we can also combine unigrams, bigrams, trigrams, and more, to form feature space.\n",
    "X = cv1.fit_transform(sents)\n",
    "X = X.toarray()\n",
    "sorted(cv1.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF or Term Frequency–Inverse Document Frequency, is a numerical statistic that’s intended to reflect how important a word is to a document. Although it’s another frequency-based method, it’s not as naive as Bag of Words.\n",
    "\n",
    "Term frequency-inverse document frequency ( TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.\n",
    "\n",
    "Term frequency denotes the frequency of a word in a document. For a specified word, it is defined as the ratio of the number of times a word appears in a document to the total number of words in the document.\n",
    "\n",
    "Inverse Document Frequency\n",
    "It measures the importance of the word in the corpus. It measures how common a particular word is across all the documents in the corpus.\n",
    "\n",
    "It is the logarithmic ratio of no. of total documents to no. of a document with a particular word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.37302199, 0.37302199,\n",
       "        0.        , 0.        , 0.49047908, 0.49047908, 0.49047908,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.45954803, 0.        , 0.        , 0.34949812, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.45954803, 0.34949812, 0.34949812, 0.        , 0.45954803,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.338348  , 0.338348  , 0.        , 0.25732238,\n",
       "        0.338348  , 0.338348  , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25732238, 0.25732238, 0.338348  , 0.        ,\n",
       "        0.338348  , 0.338348  ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sents = ['coronavirus is a highly infectious disease',\n",
    "   'coronavirus affects older people the most',\n",
    "   'older people are at high risk due to this disease']\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "X = vec.fit_transform(sents)\n",
    "X = X.toarray()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word2vec is a combination of models used to represent distributed representations of words in a corpus C.\n",
    "Word2Vec (W2V) is an algorithm that accepts text corpus as an input and outputs a vector representation for each word.\n",
    "\n",
    "There are two flavors of word2vec, such as CBOW and Skip-Gram:\n",
    "CBOW: continous bag of word:\n",
    "CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent the current word present at the output layer. \n",
    " \n",
    "Skipgam: \n",
    "Skip gram predicts the surrounding context words\n",
    "within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('law', 0.9348613619804382), ('policy', 0.9129875898361206), ('general', 0.9111494421958923), ('agriculture', 0.9049711227416992), ('discussion', 0.9039004445075989), ('education', 0.9038352370262146), ('media', 0.9017897248268127), ('biology', 0.8982501029968262), ('department', 0.8980600833892822), ('physics', 0.8975780010223389)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import abc\n",
    "\n",
    "model= gensim.models.Word2Vec(abc.sents())\n",
    "data=model.wv.most_similar('science')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
